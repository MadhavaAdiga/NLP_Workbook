{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to NLP\n"
      ],
      "metadata": {
        "id": "lm2KMz3ndcs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries\n",
        "* spaCy https://spacy.io/\n"
      ],
      "metadata": {
        "id": "ocdB6aTQdnbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy --upgrade"
      ],
      "metadata": {
        "id": "q1Q174bHdz8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.__version__"
      ],
      "metadata": {
        "id": "b2O6y09vfYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "bahv5zNMfthF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS (part-of-speech)\n",
        "* POS(part of speech): noun,adjective,verb\n",
        "* important to find named entities\n",
        "* Tokens: https://spacy.io/api/annotation#pos-tagging\n",
        "\n",
        "Tags: https://ashutoshtripathi.com/2020/04/13/parts-of-speech-tagging-and-dependency-parsing-using-spacy-nlp/"
      ],
      "metadata": {
        "id": "zp6IOnkTeCwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "from spacy import displacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# get required lang loaded on spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "0bGW8H9MpseL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string to be process by spacy\n",
        "document = nlp('I am Learning natural language processing. The course is in India.')\n",
        "\n",
        "# recognise each token in document\n",
        "for token in document:\n",
        "  # pos_ : gives part of speach in token\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "id": "_zmxasE_gk02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Legend\n",
        "\n",
        "- lemma: \"root\" of the word\n",
        "- pos: part-of-speech  \n",
        "- tag: morfological information (present, future, past)\n",
        "- dep: syntatic dependency\n",
        "- shape: lowercase, uppercasa\n",
        "- alpha: if it is alphanumeric\n",
        "- stop: if it is a stop word"
      ],
      "metadata": {
        "id": "C6unaPAhghPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in document:\n",
        "  s = \"{} | {} | {} | {} | {} | {} | {} | {} \".\\\n",
        "  format(token.text, token.pos_, token.lemma_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "lc1F4nkAi1IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for token in document:\n",
        "  # get all proper noun\n",
        "  if token.pos_ == 'PROPN':\n",
        "    print('proper noun:',token.text)\n",
        "  # get all verb\n",
        "  if token.pos_ == 'VERB':\n",
        "    print('verb:',token.text)"
      ],
      "metadata": {
        "id": "u8aR6ukkkR2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization and stemming\n",
        "\n",
        "- Lemmatization: meaning of the word based on the dictionary (morphological analysis) - extract the base word\n",
        "- Stemming: extract the root of the word\n",
        "\n",
        "* Lemmatization is prefered as it extract words with meaning through morphological analysis while this can be lost in Stemming"
      ],
      "metadata": {
        "id": "OmH7D7MdkLI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in document:\n",
        "  print(\"{} | {}\".format(token.text, token.lemma_))"
      ],
      "metadata": {
        "id": "sCkhFQBwlpvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document2 = nlp('learn learning watch watching watched')\n",
        "[token.lemma_ for token in document2]"
      ],
      "metadata": {
        "id": "EvX4YrFXmIl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization X stemming\n"
      ],
      "metadata": {
        "id": "AEoluWEMm04b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "stemmer.stem('learning')"
      ],
      "metadata": {
        "id": "trMuU15Zmye8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in document:\n",
        "  print(\"{} | {} | {}\".format(token.text, token.lemma_, stemmer.stem(token.text)))"
      ],
      "metadata": {
        "id": "Ka1OHTgnnp0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named-entity recognition (NER)\n",
        "\n",
        "- List of tags: https://towardsdatascience.com/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6\n",
        "\n",
        "* Find and classify entity in text such as\n",
        "  * people\n",
        "  * location\n",
        "  * money\n",
        "  * numbers\n",
        "* Can be used to know the subjects in the spoken language\n",
        "\n",
        "Labels -\n",
        " * GPE : geo location\n",
        " * ORG : organization\n",
        " * DATE : date\n",
        " * MONEY : money"
      ],
      "metadata": {
        "id": "Zf5CxVjeoT1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'cisco is a US company on networking, security and collaboration tech. It is located in Bangalore and revenue in 2020 was approximatly 500 billion dollars.'\n"
      ],
      "metadata": {
        "id": "-BXVcAx9pHqZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp(text)\n",
        "\n",
        "# extract entity\n",
        "for entity in document.ents:\n",
        "  print(\"{} | {}\".format(entity.text,entity.label_))"
      ],
      "metadata": {
        "id": "-JKNandmpjJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(document, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "HpDVGJjaqttI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Bill Gates was born in Seattle on 1955-10-28 and is the founder of Microsoft'\n",
        "document = nlp(text)\n",
        "\n",
        "for entity in document.ents:\n",
        "  if entity.label_ == 'PERSON':\n",
        "    print(entity.text)\n",
        "\n",
        "displacy.render(document, style = 'ent', jupyter=True)"
      ],
      "metadata": {
        "id": "WVIOyQCArGpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords\n",
        "- Words that appear very often and don't help to understand the context of the document\n",
        "\n",
        "example : it"
      ],
      "metadata": {
        "id": "i-bdbmNfrhZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(STOP_WORDS)\n",
        "\n",
        "print('it' in STOP_WORDS)\n",
        "\n",
        "print(len(STOP_WORDS))\n",
        "\n",
        "print(nlp.vocab['it'].is_stop)"
      ],
      "metadata": {
        "id": "KuY7yue2sFSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp('I am Learning natural language processing. The course is in India.')\n",
        "\n",
        "print('stop words - ')\n",
        "for token in document:\n",
        "  if nlp.vocab[token.text].is_stop:\n",
        "    print(token.text)\n",
        "\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "print('not stop words - ')\n",
        "for token in document:\n",
        "  if not nlp.vocab[token.text].is_stop:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "iY60I3Eftiyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency parsing\n",
        "\n",
        "- Parent-child relation\n",
        "\n"
      ],
      "metadata": {
        "id": "YLseqskZuLYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1 : find relation between London and Paris in text"
      ],
      "metadata": {
        "id": "8GPz6Nf_uapP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp('book a ticket from London to Paris')\n",
        "\n",
        "loc: list = []\n",
        "\n",
        "for entity in document.ents:\n",
        "  if entity.label_ == 'GPE':\n",
        "    # entity.start gives index of entity\n",
        "    loc.append(document[entity.start])\n",
        "\n",
        "\n",
        "print(loc)\n",
        "\n",
        "# identify ancestors, with which it will know how to relate\n",
        "origin = loc[0]\n",
        "print(list(origin.ancestors))\n",
        "\n",
        "dest = loc[1]\n",
        "print(list(dest.ancestors))\n",
        "\n",
        "# check for ansetory\n",
        "document[0].is_ancestor(document[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc0D3e9WuP-4",
        "outputId": "04306ef2-4020-4f1b-c324-7c013b7c1e6d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[London, Paris]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2"
      ],
      "metadata": {
        "id": "f-ZpmP_Xz3pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp('Book a table for the restaurant and a taxi to the hotel')\n",
        "\n",
        "for token in document:\n",
        "  if token.pos_ == 'NOUN':\n",
        "    print(\"{} | {} | {} | {} | {} | {} | {} | {} \".\\\n",
        "    format(token.text, token.pos_, token.lemma_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop))\n",
        "\n",
        "tasks = document[2],document[8]\n",
        "locations = document[5], document[11]\n",
        "\n",
        "print(tasks, locations)\n",
        "\n",
        "for local in locations:\n",
        "  print(\"------------\",local)\n",
        "  for obj in local.ancestors:\n",
        "    print(obj)"
      ],
      "metadata": {
        "id": "2DukVMF1vmbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for local in locations:\n",
        "  for obj in local.ancestors:\n",
        "    if obj in tasks:\n",
        "      print('Reservation of {} to the {}'.format(obj,local))\n",
        "      break"
      ],
      "metadata": {
        "id": "z6EEBMcq2JHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(document[5].children)"
      ],
      "metadata": {
        "id": "SV-sT67P3Ff2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3"
      ],
      "metadata": {
        "id": "m4jIBRIS3UbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "document = nlp('Book a table for the restaurant and a taxi to the hotel')\n",
        "\n",
        "# visualize the dependency relation\n",
        "displacy.render(document, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "ooBUBdWE3Vsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ancestor for table\n",
        "list(document[2].ancestors)"
      ],
      "metadata": {
        "id": "XhGTxsRL4JBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# children for table\n",
        "list(document[2].children)"
      ],
      "metadata": {
        "id": "qpo4RXB44VYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4"
      ],
      "metadata": {
        "id": "sZ5rwF4O4l6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp('What places can we visit in London and stay in Paris?')\n",
        "locations: list = []\n",
        "actions: list = []\n",
        "\n",
        "# get action and location lists\n",
        "for token in document:\n",
        "  if token.pos_ == 'VERB':\n",
        "    actions.append(token)\n",
        "  elif token.pos_ == 'PROPN':\n",
        "    locations.append(token)\n",
        "\n",
        "print(\"--------actions: {} \\n\".format(actions))\n",
        "print(\"--------locations: {} \\n\".format(locations))\n",
        "\n",
        "for local in locations:\n",
        "  for action in local.ancestors:\n",
        "    if action in actions:\n",
        "      print(\"{} to {}\".format(local,action))\n",
        "      break"
      ],
      "metadata": {
        "id": "1HXaWSlk4oEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(document, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "id": "Bien2XNC6bl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity between words and sentences\n",
        "\n",
        "- spaCy uses the GloVe algorithm (Global Vectors for Word Representation)\n",
        "- Original paper: https://nlp.stanford.edu/pubs/glove.pdf"
      ],
      "metadata": {
        "id": "ed-yz7QH66o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1"
      ],
      "metadata": {
        "id": "Q6Jm2D4o7OUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = nlp('hello')\n",
        "w2 = nlp('hi')\n",
        "w3 = nlp('or')\n",
        "\n",
        "# calculate similarity\n",
        "print(w1.similarity(w2))\n",
        "print(w2.similarity(w1))\n",
        "print(w1.similarity(w3))\n",
        "print(w2.similarity(w3))"
      ],
      "metadata": {
        "id": "h9iLkMkg7Rik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = nlp('When will the new movie be released?')\n",
        "text2 = nlp('The new movie will be released next month')\n",
        "text3 = nlp('What color is the car?')\n",
        "\n",
        "print(text1.similarity(text2))\n",
        "print(text1.similarity(text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYJJktUn7-M-",
        "outputId": "960a1c00-38e9-4863-f9d8-a9c3a61a75fc"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.701367333985553\n",
            "0.4782758141062681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-03274c9c2dea>:5: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(text1.similarity(text2))\n",
            "<ipython-input-104-03274c9c2dea>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(text1.similarity(text3))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2"
      ],
      "metadata": {
        "id": "7-AN0WsMFoBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nlp('cat dog horse person')\n",
        "\n",
        "for text1 in text:\n",
        "  #print('----', text1)\n",
        "  for text2 in text:\n",
        "    #print(text2)\n",
        "    similarity = text1.similarity(text2) * 100\n",
        "    print('{} is {}% similar to {}'.format(text1, similarity, text2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRkrQuHFtCd",
        "outputId": "abf8a1dc-0c56-44ee-a682-2cd4277d7f98"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat is 100.0% similar to cat\n",
            "cat is 55.56725263595581% similar to dog\n",
            "cat is 49.9476432800293% similar to horse\n",
            "cat is 19.96726244688034% similar to person\n",
            "dog is 55.56725263595581% similar to cat\n",
            "dog is 100.0% similar to dog\n",
            "dog is 66.69515371322632% similar to horse\n",
            "dog is 35.0044310092926% similar to person\n",
            "horse is 49.9476432800293% similar to cat\n",
            "horse is 66.69515371322632% similar to dog\n",
            "horse is 100.0% similar to horse\n",
            "horse is 28.581640124320984% similar to person\n",
            "person is 19.96726244688034% similar to cat\n",
            "person is 35.0044310092926% similar to dog\n",
            "person is 28.581640124320984% similar to horse\n",
            "person is 100.0% similar to person\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-04686c777bc8>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarity = text1.similarity(text2) * 100\n"
          ]
        }
      ]
    }
  ]
}