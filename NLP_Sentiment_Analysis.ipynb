{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rj2mT959dvl"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydM0qEe7-oGb"
      },
      "source": [
        "Goal of sentiment analysis is to identify text is positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y-ZsEpd9kmZ"
      },
      "outputs": [],
      "source": [
        "!pip install spacy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J-iSwASY9lLr"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIqVYQNkokPV"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Orvu9AH9mlm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jilbjReU9pnl"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "import nltk\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBc-EeEUfRtW"
      },
      "source": [
        "# Get and process Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "DIHQQz21DUFy"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/train.csv', quotechar='\"',engine=\"python\",sep=',',on_bad_lines='skip',\n",
        "                            names = ['sentiment', 'id', 'date', 'query', 'user', 'text'], encoding='latin1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AAPiVgTKgPn"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUFqUvvjMiwj",
        "outputId": "94f16fb9-8565-47c2-e779-d7368c513531"
      },
      "outputs": [],
      "source": [
        "# train_data['sentiment'].unique()\n",
        "np.unique(train_data['sentiment'], return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSyMaWv9T6Bu"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=train_data['sentiment']);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsmsFJmTe31_"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(['id', 'date', 'query', 'user'], axis=1)\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NtYYHFdfZrR"
      },
      "source": [
        "## Train and Test\n",
        "\n",
        "Goal here is to -\n",
        "* sperate similarity cloumn and sentiment column\n",
        "* to save processing choose only 30% of the data\n",
        "* split this data in training data and testing data (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaPT3Dirf3Ls"
      },
      "outputs": [],
      "source": [
        "# get only the test\n",
        "X = train_data.iloc[:, 1].values\n",
        "\n",
        "Y = train_data.iloc[:, 0].values\n",
        "\n",
        "print(X)\n",
        "\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWOT1GWrgzyI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# select only 3% of data\n",
        "x, _, y ,_ = train_test_split(X,Y,test_size=0.97)\n",
        "\n",
        "print('tweets',x.shape)\n",
        "print('sentiment',y.shape)\n",
        "\n",
        "# split data to training and testing data for ML\n",
        "# this will split - 80% training data and 20% testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "print('tweets training',x_train.shape)\n",
        "print('tweets testing',x_test.shape)\n",
        "print('sentiment training',y_train.shape)\n",
        "print('sentiment testing',y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH_jArQvkl5a"
      },
      "source": [
        "# Preprocess Data set\n",
        "\n",
        "Goal here is\n",
        "* to preprocess data by removing stop words, puntuations, numbers, spaces and single letter\n",
        "* remove '@username'\n",
        "* remove links from data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb9ai7iBlE5L"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# nlp\n",
        "\n",
        "def preprocessing(sentence: str) -> list:\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r\"@[A-Za-z0-9]+\", ' ', sentence)\n",
        "  sentence = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', sentence)\n",
        "  sentence = sentence.replace('.', '')\n",
        "  tokens = []\n",
        "  tokens = [token.text for token in nlp(sentence) if not (token.is_stop or token.like_num or token.is_punct or token.is_space or len(token) == 1)]\n",
        "  tokens = ' '.join([element for element in tokens])\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessing(\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  2 You shoulda got David Carr of Third Day to do it. ;D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_3T7R2pmmlG"
      },
      "outputs": [],
      "source": [
        "# clean training data\n",
        "x_train_cleaned = []\n",
        "\n",
        "for tweet in x_train:\n",
        "  x_train_cleaned.append(preprocessing(tweet))\n",
        "\n",
        "# check for cleaned data\n",
        "# for _ in range(10):\n",
        "#   print(x_train_cleaned[random.randint(0, len(x_train_cleaned) - 1)])\n",
        "\n",
        "print('\\n',len(x_train_cleaned))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A82Wxu9upJaA"
      },
      "outputs": [],
      "source": [
        "# clean test data\n",
        "x_test_cleaned = []\n",
        "\n",
        "for tweet in x_test:\n",
        "  x_test_cleaned.append(preprocessing(tweet))\n",
        "\n",
        "print('\\n',len(x_test_cleaned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4y5isMjnjMN"
      },
      "source": [
        "# Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKkr9tBYnAKY"
      },
      "outputs": [],
      "source": [
        "#  concatinate all the sentences into a string\n",
        "\n",
        "texts = ''\n",
        "for text in x_train_cleaned:\n",
        "  texts+=' ' + text\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "cloud = WordCloud()\n",
        "cloud = cloud.generate(texts)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1DKDhukoF_E"
      },
      "source": [
        "# Detecting languages\n",
        "\n",
        "* Goal here is to detec the languages in the data set\n",
        "* it helps with removing the stop words and clean data properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EKbdQwyoNqO"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuyTRhEEp5vY"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "\n",
        "detect(\"This is english text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTUSzufOqIsG",
        "outputId": "976ab2db-920b-45bc-ae7d-2dd6bc66c40b"
      },
      "outputs": [],
      "source": [
        "languages = []\n",
        "\n",
        "for text in x_test_cleaned:\n",
        "  if text != '':\n",
        "    languages.append(detect(text))\n",
        "\n",
        "np.unique(languages, return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-lEj14woOBR"
      },
      "source": [
        "# Sentiment analysis with NLTK\n",
        "\n",
        "with nltk we dont have to train our own algorithm\n",
        "\n",
        "Advantage -\n",
        "* very simple to use and out-of-the-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiE3wsshrg38"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "ayMqCNiysF_P"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYlPscv2sXrf"
      },
      "outputs": [],
      "source": [
        "nltk_classifier = SentimentIntensityAnalyzer()\n",
        "\n",
        "s = nltk_classifier.polarity_scores(\"I love this food\")\n",
        "print(s)\n",
        "\n",
        "s = nltk_classifier.polarity_scores(\"I hate this food\")\n",
        "print(s)\n",
        "\n",
        "s = nltk_classifier.polarity_scores(\"I have this food\")\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr1zbv64s7MR"
      },
      "outputs": [],
      "source": [
        "print(x_train_cleaned[0])\n",
        "print(\"\\n\")\n",
        "nltk_classifier.polarity_scores(x_train_cleaned[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwU9yQfqtc-h"
      },
      "outputs": [],
      "source": [
        "for sentence in x_test:\n",
        "  print(nltk_classifier.polarity_scores(sentence), ' - ', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vCzNWPftqI3"
      },
      "source": [
        "# Sentiment analysis with TF-IDF\n",
        "\n",
        "consider the following banking data -\n",
        "\n",
        "training data -\n",
        "\n",
        "|Credit History|Debts|Properties|Anual Income| Risk |\n",
        "|--------------|-----|----------|------------|------|\n",
        "|Bad|High|No|<15000|High|\n",
        "|Good|Low|Yes|>=15000 and <=40000|Low|\n",
        "\n",
        "testing data -\n",
        "\n",
        "|Credit History|Debts|Properties|Anual Income|\n",
        "|--------------|-----|----------|------------|\n",
        "|Bad|High|Yes|<15000|\n",
        "|Moderate|Low|No|>=15000 and <=40000|\n",
        "\n",
        "Classification -\n",
        "* data is cleaned\n",
        "* the data is classified into training data and testing data\n",
        "* the algorithm is trained using training data\n",
        "* when testing data is provided it needs to produce similar results\n",
        "\n",
        "Decission tree -\n",
        "* Goal is to analyse the training data and create a decission tree\n",
        "* The attributes are the nodes of the tree, i.e Credit History, Debts\n",
        "* leaf node indicate the classes, i.e low,high.moderate riks\n",
        "* the algorithm needs to perform mathematical calculations to generate this tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jZJuuyQxrzH"
      },
      "outputs": [],
      "source": [
        "# checking data\n",
        "\n",
        "print(x_train_cleaned[0:5])\n",
        "print(\"\\n\")\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_TCDJNitpUN",
        "outputId": "baa3f834-f779-4c7e-f2f2-c92778f61ec0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorize the training data set\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train_cleaned)\n",
        "\n",
        "print(x_train_tfidf.toarray().shape)\n",
        "\n",
        "# do not run in collab\n",
        "# print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKTAwA32zBtS"
      },
      "source": [
        "### Lemetization of training data\n",
        "* apply lemmetization to reduce number of columns in the vector\n",
        "* tf-idf vector columns are words/token in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Cm9MOyzAzA"
      },
      "outputs": [],
      "source": [
        "# lemmetize\n",
        "def preprocessing_lemma(s: str):\n",
        "  tokens=[]\n",
        "\n",
        "  for token in nlp(s):\n",
        "    tokens.append(token.lemma_)\n",
        "\n",
        "  tokens = ' '.join([ele for ele in tokens])\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# check function\n",
        "preprocessing_lemma('learn learned learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "RvnInHIGy4mb"
      },
      "outputs": [],
      "source": [
        "# lemetize the training data\n",
        "# x_train_cleaned_lemma = []\n",
        "\n",
        "# for tweet in x_train_cleaned:\n",
        "#   x_train_cleaned_lemma.append(preprocessing_lemma(tweet))\n",
        "\n",
        "x_train_cleaned_lemma = [preprocessing_lemma(tweet) for tweet in x_train_cleaned]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3vygVvS0Ioj",
        "outputId": "de0087b3-70a3-4fb4-c010-0d56328ef529"
      },
      "outputs": [],
      "source": [
        "# vectorize the trained data\n",
        "vectorizer = TfidfVectorizer()\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train_cleaned_lemma)\n",
        "\n",
        "#  cloumn is reduced\n",
        "print(x_train_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "Y-8Uxvzn1_3d"
      },
      "outputs": [],
      "source": [
        "# lemetize the testing data\n",
        "x_test_cleaned_lemma = [preprocessing_lemma(tweet) for tweet in x_test_cleaned]\n",
        "# # vectorize the test data\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# x_test_tfidf = vectorizer.fit_transform(x_test_cleaned_lemma)\n",
        "\n",
        "# #  cloumn is reduced\n",
        "# print(x_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFs6GYNr6jDB",
        "outputId": "63edcaf1-bcd8-49c8-ff67-0042b631d7e8"
      },
      "outputs": [],
      "source": [
        "x_test_tfidf = vectorizer.transform(x_test_cleaned_lemma)\n",
        "#  cloumn is reduced\n",
        "print(x_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "kj_TNcEw3sph",
        "outputId": "52b6427b-e684-4ce0-de80-b3fadcbe344a"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier()\n",
        "# train the classifier\n",
        "classifier.fit(x_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "z_HYx9Wk574p"
      },
      "outputs": [],
      "source": [
        "predictions = classifier.predict(x_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clztnnNr-f6z",
        "outputId": "c990b2e7-5775-4fd2-f731-2af301df612a"
      },
      "outputs": [],
      "source": [
        "predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaZj5cy0-jjx",
        "outputId": "a8e030de-63c6-476e-d05c-26973273b6b3"
      },
      "outputs": [],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHxUIzan-12_",
        "outputId": "bae25807-8617-4f58-94dc-b560abf254b1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(accuracy_score(y_test, predictions))\n",
        "print('\\n')\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(cm)\n",
        "# first row = 0\n",
        "# second row = 4\n",
        "\n",
        "# [0][0] = correct negative tweets\n",
        "# [0][1] = wrong negative tweets classified\n",
        "# [1][1] = correct positive tweets\n",
        "# [1][0] = wrong positive tweets classified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4xfXX7v_PAo",
        "outputId": "82297c3e-6887-4891-b0c0-536dfaf6dde9"
      },
      "outputs": [],
      "source": [
        "# acuracy calculation\n",
        "(3378 + 3109) / 9600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13XQ4gkw_Uw7",
        "outputId": "a85eee5b-6d1f-4a83-c1af-c2d994411048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.70      0.68      4796\n",
            "           4       0.69      0.65      0.67      4804\n",
            "\n",
            "    accuracy                           0.68      9600\n",
            "   macro avg       0.68      0.68      0.68      9600\n",
            "weighted avg       0.68      0.68      0.68      9600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CstTu0I_0zgn"
      },
      "source": [
        "# TF-IDF Sentiment analyzer Class\n",
        "\n",
        "Consolidation of the above tf-idf into a working Python Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDrzvbMx1CX9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "'''\n",
        "  Cleans and classified data in training and testing data\n",
        "'''\n",
        "class DataProcessor():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "\n",
        "  def preprocessing(self, sentence: str) -> list:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"@[A-Za-z0-9]+\", ' ', sentence)\n",
        "    sentence = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', sentence)\n",
        "    sentence = sentence.replace('.', '')\n",
        "    tokens = []\n",
        "    tokens = [token.text for token in nlp(sentence) if not (token.is_stop or token.like_num or token.is_punct or token.is_space or len(token) == 1)]\n",
        "    tokens = ' '.join([element for element in tokens])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  def preprocessing_lemma(s: str):\n",
        "    tokens=[]\n",
        "\n",
        "    for token in nlp(s):\n",
        "      tokens.append(token.lemma_)\n",
        "\n",
        "    tokens = ' '.join([ele for ele in tokens])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "  def clean(self, data: list) -> list:\n",
        "    \"\"\"\n",
        "      only cleans provided data\n",
        "    \"\"\"\n",
        "    cleaned = []\n",
        "\n",
        "    for tweet in data:\n",
        "      cleaned.append(preprocessing(tweet))\n",
        "\n",
        "    print('pre cleaned {} \\n'.format(len(data)))\n",
        "    print('cleaned',len(cleaned))\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "    def clean_lemmetize(self,data: list) -> list:\n",
        "      \"\"\"\n",
        "        cleans and lemmetize the data\n",
        "      \"\"\"\n",
        "      x_cleaned = self.clean(data)\n",
        "      x_cleaned_lemma = []\n",
        "\n",
        "      for tweet in x_cleaned:\n",
        "        x_cleaned_lemma.append(preprocessing_lemma(tweet))\n",
        "\n",
        "    def vectorize(self, cleand_data: list) -> list:\n",
        "     return self.vectorizer.fit_transform(x_train_cleaned_lemmatized)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r98WR2N4hBj"
      },
      "outputs": [],
      "source": [
        "# classify and process data\n",
        "processor = DataProcessor()\n",
        "\n",
        "X = train_data.iloc[:, 1].values\n",
        "\n",
        "Y = train_data.iloc[:, 0].values\n",
        "\n",
        "print(X)\n",
        "\n",
        "print(Y)\n",
        "\n",
        "x, _, y ,_ = train_test_split(X,Y,test_size=0.97)\n",
        "\n",
        "print('tweets',x.shape)\n",
        "print('sentiment',y.shape)\n",
        "\n",
        "# split data to training and testing data for ML\n",
        "# this will split - 80% training data and 20% testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "x_train_cleaned_lemmatized = processor.clean_lemmetize(x_train)\n",
        "x_test_cleaned_lemmatized = processor.clean_lemmetize(x_test)\n",
        "\n",
        "# vectorizer = TfidfVectorizer()\n",
        "x_train_tfidf = processor.vectorize(x_train_cleaned_lemmatized)\n",
        "x_test_tfidf =  processor.vectorize(x_test_cleaned_lemmatized)\n",
        "\n",
        "# make sure both train and test has same column count\n",
        "print(x_test_tfidf.shape)\n",
        "print('\\n')\n",
        "print(x_test_tfidf.shape)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "classifier = DecisionTreeClassifier()\n",
        "# train the classifier\n",
        "classifier.fit(x_train_tfidf, y_train)\n",
        "# predict\n",
        "predictions = classifier.predict(x_test_tfidf)\n",
        "\n",
        "# get classification report\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha9LGqYxA3L9"
      },
      "source": [
        "# Sentiment analysis with spaCy\n",
        "\n",
        "- Documentation: https://spacy.io/usage/training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yybr-dF5B-7P"
      },
      "outputs": [],
      "source": [
        "# [0][0] = text\n",
        "# [0][1] = entites\n",
        "# specify format for spacy to use\n",
        "example = [[\"this is a positive text\", {\"POSITIVE\": True, \"NEGATIVE\": False}],\n",
        "           [\"this is a negative text\", {\"POSITIVE\": False, \"NEGATIVE\": True}]]\n",
        "\n",
        "x_train_spacy = []\n",
        "for text, sentiment in zip(x_train_cleaned_lemma, y_train):\n",
        "  #print(text, sentiment)\n",
        "  if sentiment == 4:\n",
        "    dic = ({'POSITIVE': True, 'NEGATIVE': False})\n",
        "  elif sentiment == 0:\n",
        "    dic = ({'POSITIVE': False, 'NEGATIVE': True})\n",
        "  x_train_spacy.append([text, dic.copy()])\n",
        "\n",
        "x_train_spacy[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uLyW19dDdz4",
        "outputId": "8f93514d-96f5-4d2b-9845-39c044f217d0"
      },
      "outputs": [],
      "source": [
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q62tDonZDk7q",
        "outputId": "bc50f76c-7f3d-463c-b996-d8309169a6b3"
      },
      "outputs": [],
      "source": [
        "classifier_spacy = spacy.blank('en')\n",
        "classifier_spacy.pipe_names\n",
        "\n",
        "textcat = classifier_spacy.add_pipe('textcat')\n",
        "classifier_spacy.pipe_names\n",
        "\n",
        "textcat.add_label('POSITIVE')\n",
        "textcat.add_label('NEGATIVE')\n",
        "\n",
        "textcat.label_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1L52bucEUjI",
        "outputId": "c61620d8-123f-43f9-dd97-9deadc161363"
      },
      "outputs": [],
      "source": [
        "from spacy.training import Example\n",
        "\n",
        "# uses a neural network to train\n",
        "classifier_spacy.begin_training()\n",
        "\n",
        "# we need to run for several ephoc to train\n",
        "for ephoc in range(10):\n",
        "  # shuffel to better train the neural network\n",
        "  random.shuffle(x_train_spacy)\n",
        "\n",
        "  losses = {}\n",
        "  #  len of training data is 38400\n",
        "  #  by dividing to 1024 we can train with 37.5 batch of data set\n",
        "  for batch in spacy.util.minibatch(x_train_spacy, 1024):\n",
        "    # in example [0][0] = text [0][1] = entites\n",
        "    texts = [classifier_spacy.make_doc(text) for text, entities in batch]\n",
        "    # cats = categories, get entity from example\n",
        "    annotations = [{'cats' : entities}  for text, entities in batch]\n",
        "    # create new example to provide the nural network\n",
        "    examples = [Example.from_dict(doc,annotation) for doc, annotation in zip(texts, annotations)]\n",
        "\n",
        "    classifier_spacy.update(examples, losses=losses)\n",
        "\n",
        "  print(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t6l8VJzI-jK",
        "outputId": "6d89542e-3e61-418b-c7b2-ea15ec62c487"
      },
      "outputs": [],
      "source": [
        "classifier_spacy.to_disk('classifier_spacy')\n",
        "\n",
        "classifier_spacy_loaded = spacy.load('classifier_spacy')\n",
        "classifier_spacy_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSyu9kb8JVlm",
        "outputId": "e73d9f6a-486b-4796-c64d-9202776628d1"
      },
      "outputs": [],
      "source": [
        "classifier_spacy_loaded('i hate this food').cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSuTNgMaJYVT"
      },
      "outputs": [],
      "source": [
        "# create predoction in form {'POSITIVE': , 'NEGATIVE': }\n",
        "predictions = []\n",
        "for text in x_test_cleaned_lemma:\n",
        "  prediction = classifier_spacy_loaded(text)\n",
        "  predictions.append(prediction.cats)\n",
        "\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbUeT2ukKrRh",
        "outputId": "e039ea66-842a-4b2d-f55d-5a867c4572fb"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tosqkb-EJi3j",
        "outputId": "a395ac23-72dc-4346-b8e7-a7763fca1c7e"
      },
      "outputs": [],
      "source": [
        "# get only 0 or 4 from prediction based on positive and negative\n",
        "predictions2 = []\n",
        "for prediction in predictions:\n",
        "  if prediction['POSITIVE'] > prediction['NEGATIVE']:\n",
        "    predictions2.append(4)\n",
        "  else:\n",
        "    predictions2.append(0)\n",
        "predictions2 = np.array(predictions2)\n",
        "\n",
        "print(predictions2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jReh49FJng2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(accuracy_score(y_test, predictions2))\n",
        "print('\\n')\n",
        "cm = confusion_matrix(y_test, predictions2)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q52WYfeJJ1Ly",
        "outputId": "4b6c03b1-f648-40ed-da9a-1d3436d0daae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.70      0.70      4796\n",
            "           4       0.70      0.70      0.70      4804\n",
            "\n",
            "    accuracy                           0.70      9600\n",
            "   macro avg       0.70      0.70      0.70      9600\n",
            "weighted avg       0.70      0.70      0.70      9600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, predictions2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
