{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing: summarization, search, representation, and similarity"
      ],
      "metadata": {
        "id": "n_G5LGCJIFXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pttAB3D2H8Bs"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.__version__"
      ],
      "metadata": {
        "id": "sPGCRkoDJi1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "vNQiM6q-JtBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used for webscraping\n",
        "!pip install goose3"
      ],
      "metadata": {
        "id": "0Vbv6xWoJ_cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sumariztion lib\n",
        "!pip install sumy"
      ],
      "metadata": {
        "id": "oSztxckdWX2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "BWFftrIjJ5H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "from spacy import displacy\n",
        "from goose3 import Goose\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
        "\n",
        "# ML lib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# using nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# goose3 helps in extract text from websites\n",
        "g = Goose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foZM0Ct4J2D1",
        "outputId": "0b2d8212-66fe-496e-80e5-860728961904"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading texts from the Internet"
      ],
      "metadata": {
        "id": "FZhSEpkGINlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "# get data from url\n",
        "article = g.extract(url)"
      ],
      "metadata": {
        "id": "_HV75V08KdRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.title"
      ],
      "metadata": {
        "id": "3rnNTA4gKgcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gets cleand text from site removing all css\n",
        "article.cleaned_text"
      ],
      "metadata": {
        "id": "ie_gXpUoKl3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.authors"
      ],
      "metadata": {
        "id": "xNLNniR_KoYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all links in the site\n",
        "article.links"
      ],
      "metadata": {
        "id": "UKrn3hflK1v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named entity recognition\n",
        "\n",
        "Tags: https://ashutoshtripathi.com/2020/04/13/parts-of-speech-tagging-and-dependency-parsing-using-spacy-nlp/"
      ],
      "metadata": {
        "id": "yo4oni2BK_gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp(article.cleaned_text)\n",
        "\n",
        "for token in document:\n",
        "  print(token.text,'-',token.pos_)"
      ],
      "metadata": {
        "id": "MUMl7LXCLKZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(document, style = 'ent', jupyter=True)"
      ],
      "metadata": {
        "id": "15Oh5WrBLh04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in document.ents:\n",
        "  if entity.label_ == 'PERSON':\n",
        "    print(entity.text)"
      ],
      "metadata": {
        "id": "hmBeo5d6Lxae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most frequent words"
      ],
      "metadata": {
        "id": "iKX_jefqMFRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using nltk\n",
        "tokens = nltk.tokenize.word_tokenize(article.cleaned_text)\n"
      ],
      "metadata": {
        "id": "Qr9g2PSFM802"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "n9LUrXvPNPXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequncy = nltk.FreqDist(tokens)\n",
        "\n",
        "# get top 10 most frequent token\n",
        "most_common = frequncy.most_common(50)\n",
        "\n",
        "most_common"
      ],
      "metadata": {
        "id": "K58CM0qKNQmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word cloud"
      ],
      "metadata": {
        "id": "Rzx1sB7oN7Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cloud = WordCloud()\n",
        "cloud = cloud.generate(article.cleaned_text)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis('off')\n",
        "plt.imshow(cloud);"
      ],
      "metadata": {
        "id": "Fml7BzE-N8C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the texts (frequency and word cloud)"
      ],
      "metadata": {
        "id": "WHWWKr8TRppz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(s: str) -> list:\n",
        "  s = s.lower()\n",
        "  s = s.replace('.', '')\n",
        "  s = s.replace('[', '')\n",
        "  s = s.replace(']', '')\n",
        "  tokens = []\n",
        "\n",
        "  # remove stop words, puntuations, space, numbers\n",
        "  for token in nlp(s):\n",
        "    if not (token.is_stop or token.like_num or token.is_punct or token.is_space or len(token) == 1):\n",
        "      tokens.append(token.text)\n",
        "\n",
        "  tokens = ' '.join([element for element in tokens])\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "SAFn7At5RoaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess('TesT NlP it the process 1 1213 ! . ,      d ')"
      ],
      "metadata": {
        "id": "zCcfSd1kSUGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_cleaned = preprocess(article.cleaned_text)\n",
        "# article_cleaned\n",
        "\n",
        "len(article.cleaned_text),len(article_cleaned)\n",
        "\n",
        "# tokenize and get most frequent words\n",
        "tokens = nltk.tokenize.word_tokenize(article_cleaned)\n",
        "frequncy = nltk.FreqDist(tokens)\n",
        "\n",
        "# get top 10 most frequent token\n",
        "most_common = frequncy.most_common(50)\n",
        "\n",
        "most_common"
      ],
      "metadata": {
        "id": "Bku4BhViUVuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud = WordCloud()\n",
        "cloud = cloud.generate(article_cleaned)\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.axis('off')\n",
        "plt.imshow(cloud);"
      ],
      "metadata": {
        "id": "Bus0Fz9DU14u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text summarization\n",
        "\n",
        "- sumy library: https://pypi.org/project/sumy/\n",
        "\n",
        "Steps to follow -\n",
        "* **Preprocessing of text**-\n",
        "  remove stop words, puntuations, space, numbers\n",
        "\n",
        "* **Calculate Word frequency** -\n",
        "  get frequency of each word in preprocessed text\n",
        "\n",
        "* **Define Weight for word frequency** -\n",
        "  calculate the weight for each word\n",
        "  \n",
        "  Weight formule :\n",
        "  ```\n",
        "  number of times word appear / highest frequency value\n",
        "\n",
        "  or\n",
        "\n",
        "  current frecuency / highest frequency\n",
        "```\n",
        "* **Sentence tokenization** -\n",
        "  convert the original paragraph in multiple sentences(based on .),\n",
        "  NOTE - stop words are not considered\n",
        "\n",
        "* **Score for the sentence** -\n",
        "  use calculate weight for words and add them based on their appearence in the sentence\n",
        "\n",
        "* **Order the sentence** -\n",
        "  * order the sentences based on the score calculated,\n",
        "  * for this we also need to define how many sentences to be selected.\n",
        "  * larger the paragraph we need to select more sentences\n",
        "\n",
        "* **Generate Summary**\n"
      ],
      "metadata": {
        "id": "QCdxDFlQV6Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "# get data from url\n",
        "article = g.extract(url)"
      ],
      "metadata": {
        "id": "ZXOrW-1hbgPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article.cleaned_text"
      ],
      "metadata": {
        "id": "3eiWzFdKehbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use nltk to tokenize sentences in article\n",
        "original_sentences: list = []\n",
        "\n",
        "for sentence in nltk.sent_tokenize(article.cleaned_text):\n",
        "  original_sentences.append(sentence)"
      ],
      "metadata": {
        "id": "5rDnhsxLe0YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use summy to calculate high score for sentence\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "# create parser\n",
        "parser = PlaintextParser.from_string(article.cleaned_text, Tokenizer('english'))\n",
        "# get summerizer basic\n",
        "summarizer = SumBasicSummarizer()\n",
        "# create summary with 40% of best sentences\n",
        "summary_size = (len(original_sentences)/10) * 5\n",
        "summary = summarizer(parser.document, summary_size)\n",
        "\n",
        "best_sentences: list = []\n",
        "for sentence in summary:\n",
        "  best_sentences.append(str(sentence))"
      ],
      "metadata": {
        "id": "E0h04mDwfgi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "text = ''\n",
        "display(HTML(f'<h2>Summary - {article.title}</h2>'))\n",
        "\n",
        "for sentence in original_sentences:\n",
        "  #print(sentence)\n",
        "  if sentence in best_sentences:\n",
        "    text += ' ' + str(sentence).replace(sentence, f\"<mark>{sentence}</mark>\")\n",
        "  else:\n",
        "    text += ' ' + sentence\n",
        "display(HTML(f\"\"\"{text}\"\"\"))"
      ],
      "metadata": {
        "id": "7xb2oUVah2P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key word search\n"
      ],
      "metadata": {
        "id": "NWZmokYdjVEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.'"
      ],
      "metadata": {
        "id": "fslI8yDKjYcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# create a search token list\n",
        "search_string = ['artificial', 'computer']\n",
        "token_list: list = []\n",
        "\n",
        "for item in search_string:\n",
        "  token_list.append(nlp(item))"
      ],
      "metadata": {
        "id": "4vmwMn-dmRqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic word search"
      ],
      "metadata": {
        "id": "pNEsFKJJmUhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create spacy matcher\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "matcher.add('SEARCH', None, *token_list)\n",
        "\n",
        "document = nlp(string)\n",
        "matches = matcher(document)\n",
        "\n",
        "matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51UzFtpTjoL-",
        "outputId": "43a0becf-253c-4e49-8a7f-e15f21c23d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8661325627334373315, 12, 13), (8661325627334373315, 16, 17)]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document[12:13], document[16:17]"
      ],
      "metadata": {
        "id": "JoIb1VFJk-xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 words before computer and 5 words after computer\n",
        "document[12-5:13+5]"
      ],
      "metadata": {
        "id": "1XBB2-dqlA-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 words before artificial and 5 words after artificial\n",
        "document[16-5:17+5]"
      ],
      "metadata": {
        "id": "9XG88K2zlCyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word search in Wikipedia document"
      ],
      "metadata": {
        "id": "1oqnLK6BmXMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "# search words in document\n",
        "number_of_words = 50\n",
        "search_string_html = ' '.join([element for element in search_string])\n",
        "# search_string_html\n",
        "\n",
        "marked_text = ''\n",
        "display(HTML(f'<h1>{search_string_html.upper()}'))\n",
        "\n",
        "document = nlp(article.cleaned_text)\n",
        "matches = matcher(document)\n",
        "\n",
        "display(HTML(f\"\"\"<p><strong>Number of matches: </strong>{len(matches)}</p>\"\"\"))\n",
        "\n",
        "for i in matches:\n",
        "  # print(i)\n",
        "  start = i[1]-number_of_words\n",
        "  #  negative index check\n",
        "  if start < 0 :\n",
        "    start = 0\n",
        "\n",
        "  for j in range(len(token_list)):\n",
        "    # print(j, token_list[j])\n",
        "    #  i is index extracted from matcher\n",
        "    if document[i[1]:i[2]].similarity(token_list[j]) == 1.0:\n",
        "      search_text = str(token_list[j])\n",
        "      marked_text += str(document[start:i[2] + number_of_words]).replace(search_text, f\"<mark>{search_text}</mark>\")\n",
        "      marked_text += \"<br /><br />\"\n",
        "\n",
        "display(HTML(f\"\"\"<blockquote>{marked_text}</blockquote>\"\"\"))\n"
      ],
      "metadata": {
        "id": "_gRWN-pQlV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models for text representation\n",
        "Here we conver text to numerical representation for computers to understand.\n",
        "\n",
        "For complex chat bot, sentiment analysis, searching for similar document we need to convert words into numbers.\n",
        "\n",
        "* Bag Of Words\n",
        "* TF - IDF\n",
        "\n",
        "[Difference between BOG and TF-IDF](https://mayurji.github.io/blog/2021/09/20/Tf-Idf#:~:text=Unlike%2C%20bag%2Dof%2Dwords,documents%20this%20word%20appears%20in.&text=N%20is%20the%20total%20number,known%20as%20inverse%20document%20frequency.)"
      ],
      "metadata": {
        "id": "P1jsJfeXknS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words\n",
        "A simple way to represent sentences/words.\n",
        "In NLP we need to represent words in a numerical way as computer and algorithum understands numbers\n",
        "\n",
        "For complex chat bot, sentiment analysis, searching for similar document we need to convert words into numbers."
      ],
      "metadata": {
        "id": "0bLFcNy7jRmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "consider example sentences -\n",
        "1. This is the first document\n",
        "2. this document is the second document\n",
        "3. And this is third one\n",
        "4. Is this the first document?\n",
        "\n",
        "| Sentence# | and | document | first | is | one | second | the | third | these |\n",
        "|----|-----|----------|-------|----|-----|--------|-----|-------|------|\n",
        "| 1 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 1 |\n",
        "| 2 | 0 | 2 | 0 | 1 | 0 | 1 | 1 | 0 | 1 |\n",
        "| 3 | 1 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 1 |\n",
        "| 4 | 0 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 1 |\n",
        "\n",
        "- identify unique words in each sentences\n",
        "- fill the table with the frequency in sentences\n",
        "- now this is simple to represent the bag of word format which is a 2D matrix\n"
      ],
      "metadata": {
        "id": "_dTu76h_k3zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drawback of bag of words represntation\n",
        "* We only count number of times the unique words appear in a single sentences.\n",
        "* This representation is not very good as frequent words dont innate the document and may not represent much information about the context.\n",
        "\n",
        "Example a single word may appear 100 times and other may appear less often.\n",
        "* Higher weight is given to the word that appear most often.\n",
        "* Less weight is given to word that appear less often.\n",
        "* Other problem is longer sentences will have greater weight than shorter sentences."
      ],
      "metadata": {
        "id": "IVLh8p8KfqaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML lib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = ['This is the first document.',\n",
        "             'This document is the second document.',\n",
        "             'And this is the third one.',\n",
        "             'Is this the first document?']\n",
        "\n",
        "# create vecotrizer\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# represent in table\n",
        "header = vectorizer.get_feature_names_out()\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.get_xaxis().set_visible(False)\n",
        "ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.box(on=None)\n",
        "the_table = plt.table(colLabels=header,cellText=x.toarray(),loc='center');\n",
        "the_table.scale(2, 2.5)"
      ],
      "metadata": {
        "id": "nrLwiYfpjSRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize Wikipedia document"
      ],
      "metadata": {
        "id": "wSWD6Rk6qEir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML lib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "# get data from url\n",
        "article = g.extract(url)\n",
        "\n",
        "# use nltk to tokenize sentences in article\n",
        "original_sentences: list = []\n",
        "\n",
        "for sentence in nltk.sent_tokenize(article.cleaned_text):\n",
        "  original_sentences.append(sentence)\n",
        "\n",
        "# vectorize wiki\n",
        "vectorizer = CountVectorizer()\n",
        "x_sentences = vectorizer.fit_transform(original_sentences)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "id": "40BAR4fcoIOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF (Time Frequency - Inverse Document Frequency)\n",
        "\n",
        "* Created to overcome the limitations of Bag of words algorithm.\n",
        "* It considers the frequency of words appearing in all sentences of document, where as in bag of words frequecy is consodered only for one sentence.\n",
        "\n",
        "* Calculating TF is also called as Normalization, where all terms are considered as equally important.\n",
        "* In IDF we increase weight of less frequent words and decrease weight of most frequent words.\n",
        "\n",
        "TF Formule -\n",
        "```\n",
        "TF = Number of timers term T appears in document / total number of terms in document\n",
        "```\n",
        "IDF Formule -\n",
        "```\n",
        "IDF = 1 + log(Total number of document / number of documents term T appeared)\n",
        "```\n",
        "Calcuate TF-IDF -\n",
        "```\n",
        "  TF * IDF\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "consider previous example sentences -\n",
        "1. This is the first document\n",
        "2. this document is the second document\n",
        "3. And this is third one\n",
        "4. Is this the first document ?"
      ],
      "metadata": {
        "id": "vAQ8-Fb0snkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate TF (Normalization)\n",
        "\n",
        "Steps -\n",
        "* create table similar to bag of words modeling\n",
        "* get number of tokens/terms in each sentences\n",
        "* use TF formule to fill the table of TF\n",
        "\n",
        "<br>\n",
        "\n",
        "**Bag of words representation** -\n",
        "\n",
        "| Sentence# | and | document | first | is | one | second | the | third | these |\n",
        "|----|-----|----------|-------|----|-----|--------|-----|-------|------|\n",
        "| 1 | - | 1 | 1 | 1 | - | - | 1 | - | 1 |\n",
        "| 2 | - | 2 | - | 1 | - | 1 | 1 | - | 1 |\n",
        "| 3 | 1 | - | - | 1 | 1 | - | 1 | 1 | 1 |\n",
        "| 4 | - | 1 | 1 | 1 | - | - | 1 | - | 1 |\n",
        "\n",
        "**Number of token/ term in sentences** -\n",
        "\n",
        "Sentence 1 - 5 ,\n",
        "Sentence 2 - 6 ,\n",
        "Sentence 3 - 6 ,\n",
        "Sentence 4 - 5\n",
        "\n",
        "**Apply TF formule to table**\n",
        "```\n",
        "TF = Number of timers term T appears in document / total number of terms in document\n",
        "```\n",
        "| Sentence# | and | document | first | is | one | second | the | third | these |\n",
        "|----|-----|----------|-------|----|-----|--------|-----|-------|------|\n",
        "| 1 | - | 0.20 | 0.20 | 0.20 | - | - | 0.20 | - | 0.20 |\n",
        "| 2 | - | 0.33 | - | 0.16 | - | 0.16 | 0.16 | - | 0.16 |\n",
        "| 3 | 0.16 | - | - | 0.16 | 0.16 | - | 0.16 | 0.16 | 0.16 |\n",
        "| 4 | - | 0.20 | 0.20 | 0.20 | - | - | 0.20 | - | 0.20 |\n"
      ],
      "metadata": {
        "id": "Gh3JGw5EmaJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate IDF\n",
        "\n",
        "Steps -\n",
        "* Get total number of sentences in document.\n",
        "* Get the term T which we need to calculate.\n",
        "* Get the sentences in which term T appear.\n",
        "* apply formule -\n",
        "```\n",
        "IDF = 1 + log(Total number of document / number of documents term T appeared)\n",
        "```\n",
        "\n",
        "Example consider term document in the above sentences.\n",
        "* it appears in sentence #1, #2, #4\n",
        "* document appeares in each document appears total of 3 times,\n",
        "* **NOTE - even thought term document appeares 2 times in the 2 sentence, it will be treated a 1 time, as we are interested in total no of sentence the term appear and not how many times it appear in each sentence**\n",
        "* apply formule -\n",
        "```\n",
        "  IDF = 1 + log(4/3)\n",
        "  IDF = 1.28\n",
        "```"
      ],
      "metadata": {
        "id": "Ks5dIGHporTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate TF-IDF value\n",
        "\n",
        "to calcuate TF-IDF -\n",
        "```\n",
        "  TF * IDF\n",
        "```\n",
        "\n",
        "Lets create table for only two terms for now, but in reality it needs to be done for all words\n",
        "\n",
        "| term | Sentence #1 | Sentence #2 | Sentence #3 | Sentence #4 |\n",
        "|------|-------------|-------------|-------------|-------------|\n",
        "| document| 0.20 x 1.28 = 0.25 | 0.33 x 1.28 = 0.42 | 0 | 0.20 x 1.28 = 0.25 |\n",
        "| first   | 0.20 x 1.69 = 0.33 | 0                  | 0 | 0.20 x 1.69 = 0.33 |"
      ],
      "metadata": {
        "id": "QvqSxnBMrduJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML lib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = ['This is the first document.',\n",
        "             'This document is the second document.',\n",
        "             'And this is the third one.',\n",
        "             'Is this the first document?']\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "x = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# vectorizer.idf_\n",
        "\n",
        "# represent in table\n",
        "header = vectorizer.get_feature_names_out()\n",
        "vectorizedValue = x.toarray()\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.get_xaxis().set_visible(False)\n",
        "ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.box(on=None)\n",
        "the_table = plt.table(colLabels=header,cellText=vectorizedValue,loc='center');\n",
        "the_table.scale(3, 3.5)"
      ],
      "metadata": {
        "id": "UepJh82Ks2xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize Wikipedia document"
      ],
      "metadata": {
        "id": "68TxC7OxugFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML lib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "# get data from url\n",
        "article = g.extract(url)\n",
        "\n",
        "# use nltk to tokenize sentences in article\n",
        "original_sentences: list = []\n",
        "\n",
        "for sentence in nltk.sent_tokenize(article.cleaned_text):\n",
        "  original_sentences.append(sentence)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "# we need to preprocess text for better results\n",
        "x_sentences = vectorizer.fit_transform(original_sentences)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(vectorizer.idf_)\n",
        "print(x_sentences.toarray())"
      ],
      "metadata": {
        "id": "BorIpAclufyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine similarity\n",
        "\n",
        "- Link: https://en.wikipedia.org/wiki/Cosine_similarity\n",
        "- Step by step calculation: https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/"
      ],
      "metadata": {
        "id": "SkWDIjokvaD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML lib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "# get data from url\n",
        "article = g.extract(url)\n",
        "\n",
        "# use nltk to tokenize sentences in article\n",
        "original_sentences: list = []\n",
        "\n",
        "for sentence in nltk.sent_tokenize(article.cleaned_text):\n",
        "  original_sentences.append(sentence)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "# we need to preprocess text for better results\n",
        "x_sentences = vectorizer.fit_transform(original_sentences)\n",
        "\n",
        "x_sentences.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvsMYQFdwxrQ",
        "outputId": "bb8fdd08-66f5-49b2-aefa-925a721955ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 848)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#  get two sentence from wiki\n",
        "x_test_similarity = x_sentences[0:3]\n",
        "x_test_similarity: np.ndarray = x_test_similarity.toarray()\n",
        "\n",
        "# concatinate sentence 1 to end of array to have an extra copy\n",
        "x_test_similarity = np.concatenate((x_test_similarity,x_test_similarity[0].reshape(1,-1)), axis=0)\n",
        "\n",
        "print(x_test_similarity)\n",
        "print(x_test_similarity.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Os2MNUbcw6C-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity between Sentence 1 and 2\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculate similarity b/w sentence 1 and sentence 2\n",
        "# reshape to get matrix from vector\n",
        "s = cosine_similarity(x_test_similarity[0].reshape(1,-1), x_test_similarity[1].reshape(1,-1))\n",
        "\n",
        "print('similarity between sentences 1 and 2 \\n',original_sentences[0],'\\n',original_sentences[1])\n",
        "print('is equal to')\n",
        "print(s)"
      ],
      "metadata": {
        "id": "hJJYu9qdziRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity between Sentence 2 and 3\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculate similarity b/w sentence 1 and sentence 2\n",
        "# reshape to get matrix from vector\n",
        "s = cosine_similarity(x_test_similarity[1].reshape(1,-1), x_test_similarity[2].reshape(1,-1))\n",
        "\n",
        "print('similarity between sentences 1 and 3 \\n',original_sentences[1],'\\n',original_sentences[2])\n",
        "print('is equal to')\n",
        "print(s)"
      ],
      "metadata": {
        "id": "SIulL8PDxZx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity between Sentence 1 and 4 (4 is same as 1)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculate similarity b/w sentence 1 and sentence 2\n",
        "# reshape to get matrix from vector\n",
        "s = cosine_similarity(x_test_similarity[0].reshape(1,-1), x_test_similarity[3].reshape(1,-1))\n",
        "\n",
        "print('similarity between sentences 1 and 4 \\n {} \\n {}'.format(original_sentences[0],original_sentences[3]))\n",
        "print('is equal to')\n",
        "print(s)"
      ],
      "metadata": {
        "id": "TG9uZ7muyUhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity between Sentence 1 and all\n",
        "\n",
        "cosine_similarity(x_test_similarity[0].reshape(1,-1), x_test_similarity)"
      ],
      "metadata": {
        "id": "wt2ci6060gRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating a chatbot"
      ],
      "metadata": {
        "id": "HN_bjHxu0pV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# ML lib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "welcome_words_inputs = (\"hello\",\"hi\",\"hey\")\n",
        "welcome_words_outputs = ('hey', 'hello', 'hi', 'how are you?', 'welcome', 'how are you doing?')\n",
        "\n",
        "class Chatbot() :\n",
        "\n",
        "  def __init__(self):\n",
        "    url = 'https://en.wikipedia.org/wiki/Natural_language_processing'\n",
        "    # get data from url\n",
        "    article = g.extract(url)\n",
        "\n",
        "    # use nltk to tokenize sentences in article\n",
        "    self.original_sentences: list = []\n",
        "\n",
        "    for sentence in nltk.sent_tokenize(article.cleaned_text):\n",
        "      self.original_sentences.append(sentence)\n",
        "\n",
        "  def preprocess(self, s: str) -> list:\n",
        "    s = s.lower()\n",
        "    s = s.replace('.', '')\n",
        "    s = s.replace('[', '')\n",
        "    s = s.replace(']', '')\n",
        "    tokens = []\n",
        "\n",
        "    # remove stop words, puntuations, space, numbers\n",
        "    for token in nlp(s):\n",
        "      if not (token.is_stop or token.like_num or token.is_punct or token.is_space or len(token) == 1):\n",
        "        tokens.append(token.text)\n",
        "\n",
        "    tokens = ' '.join([element for element in tokens])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "  def welcom_message(self, text: str) -> str:\n",
        "    for word in text.split():\n",
        "      if word.lower() in welcome_words_inputs:\n",
        "        return random.choice(welcome_words_outputs)\n",
        "\n",
        "\n",
        "  def answer(self, question: str, threshold=0.3):\n",
        "    cleaned_sentences = []\n",
        "    # clean the wiki document sentences\n",
        "    for sentences in self.original_sentences:\n",
        "      cleaned_sentences.append(self.preprocess(sentences))\n",
        "\n",
        "    # print(cleaned_sentences)\n",
        "\n",
        "    chatbot_answer = ''\n",
        "    # clean user question\n",
        "    question = self.preprocess(question)\n",
        "\n",
        "    # append question to get it vectorized\n",
        "    cleaned_sentences.append(question)\n",
        "    # print(cleaned_sentences[-1])\n",
        "\n",
        "    # vectorize\n",
        "    tfidf = TfidfVectorizer()\n",
        "    x_sentences = tfidf.fit_transform(cleaned_sentences)\n",
        "    # print(x_sentences.toarray()[-1])\n",
        "\n",
        "    similarity = cosine_similarity(x_sentences[-1],x_sentences)\n",
        "    # print(similarity)\n",
        "\n",
        "    # get sentence with highest similarity\n",
        "    sentence_index = similarity.argsort()[0][-2]\n",
        "    # print(sentence_index)\n",
        "    # print(similarity[0][sentence_index])\n",
        "\n",
        "    if similarity[0][sentence_index] < threshold:\n",
        "      chatbot_answer += 'Sorry no answer was found!'\n",
        "    else:\n",
        "      chatbot_answer += original_sentences[sentence_index]\n",
        "\n",
        "    return chatbot_answer\n"
      ],
      "metadata": {
        "id": "OtWhGUPn0oa6"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = Chatbot()\n",
        "\n",
        "c.answer(\"what is natural language processing\")"
      ],
      "metadata": {
        "id": "PR_iW82F4PxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = Chatbot()\n",
        "\n",
        "c.answer(\"who is alan turing\")"
      ],
      "metadata": {
        "id": "mz3xwppz7uHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = Chatbot()\n",
        "\n",
        "c.answer(\"what is NLP\",0.1)"
      ],
      "metadata": {
        "id": "3_c7adMZ715Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = Chatbot()\n",
        "cont = True\n",
        "print('Hello! I am a chatbot and I will answer your questions about natural language processing')\n",
        "\n",
        "while cont == True:\n",
        "  user_text = input()\n",
        "\n",
        "  if user_text != 'quit':\n",
        "    if c.welcom_message(user_text) != None:\n",
        "      print('Chatbot: ' + c.welcom_message(user_text))\n",
        "    else:\n",
        "      print('Chatbot:')\n",
        "      print(c.answer(user_text))\n",
        "  else:\n",
        "    cont = False\n",
        "    print('Chatbot: Bye! I will see you soon')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Tvzkoohk8K9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}