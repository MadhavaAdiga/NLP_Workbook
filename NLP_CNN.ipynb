{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vIG2rRCnc2v"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "---\n",
        "\n",
        "It is basically a neural network, which take image as input and outputs a Label.\n",
        "Example - taking a image and identifying if it contains a aeroplane in it.\n",
        "\n",
        "Steps for CNN -\n",
        "* **Convolution** : create \"feature detectors\" that go through the image, end up with list of feature maps which tell where this feature appears\n",
        "* **Max Pooling** : apply maximum funtion to feature maps, so it can be made smaller to get more performance\n",
        "* **Falttening** : create vector out of feature maps\n",
        "* **Full Connection** : create a full neural network\n",
        "\n",
        "Links - \n",
        "- https://dennybritz.com/posts/wildml/understanding-convolutional-neural-networks-for-nlp/\n",
        "- https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_XB1mV-u8dy"
      },
      "source": [
        "## CNN for Text\n",
        "\n",
        "We need to create Sentences to a Matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGbYjND00F9"
      },
      "source": [
        "# Import Dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "daq1-oxHnLoP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJtnYNC74P7d",
        "outputId": "74139a32-0c12-4095-dcf2-afe8c602ad82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "import tensorflow_datasets as tfds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "116n01Qh59Fh"
      },
      "source": [
        "# DataProcessing sentiment_140 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "314EXpqZ6De5"
      },
      "source": [
        "## load dataset file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqEWw-0A6BSW",
        "outputId": "819119ba-7d79-4a47-edf4-27afa6bd8ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hmxCeKv8PZ9G"
      },
      "outputs": [],
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\",\"query\", \"user\", \"text\"]\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/sentiment_140.csv\", header=None, names=cols, engine=\"python\",encoding=\"latin1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-SEUHZyQM2i"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJvwGIPtS6y1"
      },
      "source": [
        "# PreProcess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gbF6W4hlRHzW"
      },
      "outputs": [],
      "source": [
        "# get train and test data\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPE7uH8xR9ho"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTVEZLE2QeP5"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "np.unique(data['sentiment'], return_counts=True)\n",
        "sns.countplot(x=data['sentiment']);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3lhbJ1WSCsb"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentiment = data.iloc[:, 0].values\n",
        "text = data.iloc[:, 1].values\n",
        "\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hIAzF-GySwed"
      },
      "outputs": [],
      "source": [
        "def clean_data(s):\n",
        "  s = BeautifulSoup(s, \"lxml\").get_text()\n",
        "  s = re.sub(r\"@[A-Za-z0-9]+\", ' ', s)\n",
        "  s = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?']\", ' ', s)\n",
        "  s = re.sub(r\" +\", ' ', s)\n",
        "\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu-oA8nHTu9R"
      },
      "outputs": [],
      "source": [
        "data_clean = [clean_data(s) for s in text]\n",
        "print(data_clean[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao07zVC6VcTL"
      },
      "outputs": [],
      "source": [
        "# convert sentiment 0,4 to 0,1\n",
        "print(\"before\",set(sentiment))\n",
        "sentiment[sentiment == 4] = 1\n",
        "print(\"after\",set(sentiment))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4L0TQ_LZUtM"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "From sentence get list of numbers, where each number corresponds to a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Oqvfk6wAZW-I"
      },
      "outputs": [],
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    data_clean, target_vocab_size=2**16\n",
        ")\n",
        "\n",
        "data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNyNk44fJC6"
      },
      "source": [
        "# Padding\n",
        "\n",
        "Goal is to Pad the tokenized data with 0 at the end of the sentences inorder to make the inputs of equal length.\n",
        "\n",
        "For training AI the inputs are provided as batch and for this the inputs should be of same length.\n",
        "\n",
        "0 dosent have any meaning as tokenizer will not have any word associated to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uU4Q9DJgfvqK"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = max([len(sentence) for sentence in data_inputs])\n",
        "\n",
        "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    data_inputs,\n",
        "    value=0,\n",
        "    padding=\"post\",\n",
        "    maxlen=MAX_LEN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbx3UwC5VXJG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data to training and testing data for ML\n",
        "# this will split - 80% training data and 20% testing data\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_inputs,sentiment,test_size=0.2)\n",
        "\n",
        "print('tweets training',x_train.shape)\n",
        "print('tweets testing',x_test.shape)\n",
        "print('sentiment training',y_train.shape)\n",
        "print('sentiment testing',y_test.shape)\n",
        "\n",
        "print(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZnovAy3gjnz"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sA7PNocggi9e"
      },
      "outputs": [],
      "source": [
        "class DeepCNN(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, emb_dim=128, nb_filters=50, FFN_units=512, nb_classes=2, dropout_rate=0.1, training=False, name=\"dcnn\"):\n",
        "    super(DeepCNN,self).__init__(name=name)\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_1 = layers.GlobalMaxPool1D()\n",
        "\n",
        "    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_2 = layers.GlobalMaxPool1D()\n",
        "\n",
        "    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding=\"valid\", activation=\"relu\")\n",
        "    self.pool_3 = layers.GlobalMaxPool1D()\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    if nb_classes == 2:\n",
        "      self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
        "    else:\n",
        "      self.last_dense = layers.Dense(units=nb_classes, activation=\"softmax\")\n",
        "\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    x = self.embedding(inputs)\n",
        "\n",
        "    x_1 = self.bigram(x)\n",
        "    x_1 = self.pool_1(x_1)\n",
        "\n",
        "    x_2 = self.trigram(x)\n",
        "    x_2 = self.pool_2(x_2)\n",
        "\n",
        "    x_3 = self.fourgram(x)\n",
        "    x_3 = self.pool_3(x_3)\n",
        "\n",
        "    merged = tf.concat([x_1,x_2,x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "    merged = self.dense_1(merged)\n",
        "    merged = self.dropout(merged, training)\n",
        "\n",
        "    output = self.last_dense(merged)\n",
        "\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vQPZrHZl1JU"
      },
      "source": [
        "# Application\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHcsVFQXmIGH"
      },
      "source": [
        "## Config\n",
        "goal is to create global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LXzTr4bRlz7W"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "EMB_DIM = 200\n",
        "NB_FILTERS = 100\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = len(set(y_train))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7A2UNslmMTf"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SSXNuvxRmLnb"
      },
      "outputs": [],
      "source": [
        "dcnn = DeepCNN(vocab_size=VOCAB_SIZE,\n",
        "                emb_dim=EMB_DIM,\n",
        "                nb_filters=NB_FILTERS,\n",
        "                FFN_units=FFN_UNITS,\n",
        "                nb_classes=NB_CLASSES,\n",
        "                dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "if NB_CLASSES == 2:\n",
        "    dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "else:\n",
        "    dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GZDswIRunULH"
      },
      "outputs": [],
      "source": [
        "# create check point to save model\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(DeepCNN=dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjQfAOm8oAyD",
        "outputId": "da97e936-0b1b-4f70-f25e-329de5be8a46"
      },
      "outputs": [],
      "source": [
        "dcnn.fit(x_train,\n",
        "         y_train,\n",
        "         batch_size=BATCH_SIZE,\n",
        "         epochs=NB_EPOCHS)\n",
        "ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYuYywNi-61R"
      },
      "outputs": [],
      "source": [
        "results = dcnn.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiDpMcuQ_Hh1"
      },
      "outputs": [],
      "source": [
        "dcnn(np.array([tokenizer.encode(\"You are so nice\")]), training=False).numpy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
