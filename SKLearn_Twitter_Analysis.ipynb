{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khKpQTUJltZQ"
      },
      "outputs": [],
      "source": [
        "!pip install spacy --upgrade\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependencies"
      ],
      "metadata": {
        "id": "eiLMXMPBmU8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "from langdetect import detect\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "BkILy0llmX0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('TensorFlow version: ', tf.__version__)"
      ],
      "metadata": {
        "id": "jIcTk982mw3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rVKNJyiOnOEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\",\"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv('/content/drive/MyDrive/training.1600000.processed.noemoticon.csv', engine='python',\n",
        "                   encoding='latin1', header=None,names=cols)"
      ],
      "metadata": {
        "id": "wIuBbd9wnA-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "id": "k2Z6gk9YoapL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop([\"id\", \"date\",\"query\", \"user\"], axis=1, inplace=True)\n",
        "data[:5]"
      ],
      "metadata": {
        "id": "QnnKho9moqog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(data['sentiment'], return_counts=True)"
      ],
      "metadata": {
        "id": "xkThmaUHpQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.iloc[:,1].values\n",
        "y = data['sentiment'].values\n",
        "\n",
        "print(x)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "O4o345zpqdTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['text']\n",
        "y = data['sentiment']\n",
        "\n",
        "x, _, y ,_ = train_test_split(x,y,test_size=0.97)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "5SnmVSeZpXUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('tweets training',x_train.shape)\n",
        "print('tweets testing',x_test.shape)\n",
        "print('sentiment training',y_train.shape)\n",
        "print('sentiment testing',y_test.shape)"
      ],
      "metadata": {
        "id": "VgKgpTENq7wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "P5Ex0ixdtZ9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess(sentence: str) -> list:\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r\"@[A-Za-z0-9]+\", ' ', sentence)\n",
        "  sentence = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', sentence)\n",
        "  sentence = sentence.replace('.', '')\n",
        "  tokens = []\n",
        "  tokens = [token.text for token in nlp(sentence) if not (token.is_stop or token.like_num or token.is_punct or token.is_space or len(token) == 1)]\n",
        "  tokens = ' '.join([element for element in tokens])\n",
        "\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "2UwpuEr3rDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cleaned = []\n",
        "\n",
        "for tweet in x_train:\n",
        "  x_train_cleaned.append(preprocess(tweet))\n",
        "\n",
        "print('\\n',len(x_train_cleaned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ41Nh87tEo2",
        "outputId": "32fb79a5-45c3-4128-fba4-d79807c5ab7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 38400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_cleaned = []\n",
        "\n",
        "for tweet in x_test:\n",
        "  x_test_cleaned.append(preprocess(tweet))\n",
        "\n",
        "print('\\n',len(x_test_cleaned))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVRp35N44mJV",
        "outputId": "779d09a7-3ae2-42f4-f2e8-96b286622a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatize"
      ],
      "metadata": {
        "id": "QXX_IBIztcD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(s:str) -> list:\n",
        "  tokens = [token.lemma_ for token in nlp(s)]\n",
        "\n",
        "  tokens = ' '.join([ele for ele in tokens])\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "ZSHJqBUptecB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_lemma = [lemmatize(x) for x in x_train_cleaned]\n",
        "\n",
        "print('\\n',len(x_train_lemma))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPkYAAcBvKKQ",
        "outputId": "462a281c-17dc-4ad9-f7a4-d4104b2594c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 38400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_lemma = [lemmatize(x) for x in x_test_cleaned]\n",
        "\n",
        "print('\\n',len(x_test_lemma))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKktJ8bHIVHT",
        "outputId": "5ddef3fe-d00e-4ca2-c666-bb5148063209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "x_train_tfidf = vectorizer.fit_transform(x_train_lemma)"
      ],
      "metadata": {
        "id": "sUgFyaY4y-5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RS6rceszyBb",
        "outputId": "742cf626-e6a2-4586-cef3-70c2eff548d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38400, 30012)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_tfidf = vectorizer.transform(x_test_lemma)\n",
        "print(x_test_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGeZXnlCIqZM",
        "outputId": "70d80881-35b8-46e0-e5ff-0ab8fdb14cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9600, 30012)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SKLearn Multi Perceptron Classifier\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#:~:text=Alpha%20is%20a%20parameter%20for,that%20appears%20with%20lesser%20curvatures.\n",
        "\n"
      ],
      "metadata": {
        "id": "nwdd7Pegz78J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Alpha** : is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures\n",
        "- **hidden_layer_sizes** : This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple. Thus the length of tuple denotes the total number of hidden layers in the network.\n",
        "- **max_iter**: It denotes the number of epochs.\n",
        "- **activation**: The activation function for the hidden layers.\n",
        "- **solver**: This parameter specifies the algorithm for weight optimization across the nodes.\n",
        "- **random_state**: The parameter allows to set a seed for reproducing the same results"
      ],
      "metadata": {
        "id": "clHvzqFDRDNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ Nh=Ns(α∗(Ni+No)) $\n",
        "\n",
        "- Ni = number of input neurons.\n",
        "- No = number of output neurons.\n",
        "- Ns = number of samples in training data set.\n",
        "- α = an arbitrary scaling factor usually 2-10"
      ],
      "metadata": {
        "id": "wQvWhkbJfnWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classifier = MLPClassifier(alpha=1e-5,\n",
        "                           hidden_layer_sizes=(7,3),\n",
        "                           max_iter=200,\n",
        "                           activation = 'relu',\n",
        "                           solver='adam',\n",
        "                           random_state=1)\n",
        "\n",
        "classifier.fit(x_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "abqvL1fmz6wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(classifier.loss_curve_)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dVvlQGE4UhZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[17:18]"
      ],
      "metadata": {
        "id": "KwZi9KxKHcMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.predict(x_test_tfidf[17:18])"
      ],
      "metadata": {
        "id": "so0rfz0K9EpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "onOeCpp6P3_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = classifier.predict(x_test_tfidf)"
      ],
      "metadata": {
        "id": "dVweEF_WP3W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(accuracy_score(y_test, predictions))\n",
        "print('\\n')\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73HLqERCP9M0",
        "outputId": "9f885da6-d578-4896-f5f8-deec3539b012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6816666666666666\n",
            "\n",
            "\n",
            "[[3382 1459]\n",
            " [1597 3162]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDCWi1yCQBEd",
        "outputId": "330abfb2-88d7-4336-ce6d-aca706c5e5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.70      0.69      4841\n",
            "           4       0.68      0.66      0.67      4759\n",
            "\n",
            "    accuracy                           0.68      9600\n",
            "   macro avg       0.68      0.68      0.68      9600\n",
            "weighted avg       0.68      0.68      0.68      9600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "5kDY5g3J3XE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rMEQU7n3Yj0",
        "outputId": "9ce129e9-2bd6-4385-db00-a65d97cdb5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = [[\"this is a positive text\", {\"POSITIVE\": True, \"NEGATIVE\": False}],\n",
        "           [\"this is a negative text\", {\"POSITIVE\": False, \"NEGATIVE\": True}]]\n",
        "\n",
        "x_train_spacy = []\n",
        "for text, sentiment in zip(x_train_lemma, y_train):\n",
        "  #print(text, sentiment)\n",
        "  if sentiment == 4:\n",
        "    dic = ({'POSITIVE': True, 'NEGATIVE': False})\n",
        "  elif sentiment == 0:\n",
        "    dic = ({'POSITIVE': False, 'NEGATIVE': True})\n",
        "  x_train_spacy.append([text, dic.copy()])\n",
        "\n",
        "x_train_spacy[0:5]"
      ],
      "metadata": {
        "id": "c3hCIa-N4qrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_spacy = spacy.blank('en')\n",
        "# classifier_spacy.pipe_names\n",
        "\n",
        "textcat = classifier_spacy.add_pipe('textcat')\n",
        "# classifier_spacy.pipe_names\n",
        "\n",
        "textcat.add_label('POSITIVE')\n",
        "textcat.add_label('NEGATIVE')\n",
        "\n",
        "textcat.label_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_zrEcQW393o",
        "outputId": "79495d43-58a5-4d7a-e1c0-da2fa832b371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('POSITIVE', 'NEGATIVE')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# uses a neural network to train\n",
        "classifier_spacy.begin_training()\n",
        "\n",
        "for ephoc in range(20):\n",
        "  random.shuffle(x_train_spacy)\n",
        "\n",
        "  losses = {}\n",
        "  for batch in spacy.util.minibatch(x_train_spacy, 1024):\n",
        "    # in example [0][0] = text [0][1] = entites\n",
        "    texts = [classifier_spacy.make_doc(text) for text, entities in batch]\n",
        "    # cats = categories, get entity from example\n",
        "    annotations = [{'cats' : entities}  for text, entities in batch]\n",
        "    # create new example to provide the nural network\n",
        "    examples = [Example.from_dict(doc,annotation) for doc, annotation in zip(texts, annotations)]\n",
        "\n",
        "    classifier_spacy.update(examples, losses=losses)\n",
        "\n",
        "  print(losses)"
      ],
      "metadata": {
        "id": "KVnr47tQ407M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for text in x_test_lemma:\n",
        "  prediction = classifier_spacy(text)\n",
        "  predictions.append(prediction.cats)\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "VUGSNDKk8sHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = []\n",
        "for prediction in predictions:\n",
        "  if prediction['POSITIVE'] > prediction['NEGATIVE']:\n",
        "    predictions2.append(4)\n",
        "  else:\n",
        "    predictions2.append(0)\n",
        "predictions2 = np.array(predictions2)\n",
        "\n",
        "print(predictions2)"
      ],
      "metadata": {
        "id": "FdPZJNlU8slD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "print(accuracy_score(y_test, predictions2))\n",
        "print('\\n')\n",
        "cm = confusion_matrix(y_test, predictions2)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "v7HEAtM38xA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, predictions2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp2wbb2V8yrO",
        "outputId": "33b542c9-638a-4c11-b184-c2dd9264d91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.69      0.69      4841\n",
            "           4       0.69      0.69      0.69      4759\n",
            "\n",
            "    accuracy                           0.69      9600\n",
            "   macro avg       0.69      0.69      0.69      9600\n",
            "weighted avg       0.69      0.69      0.69      9600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}